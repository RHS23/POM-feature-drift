{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c72c3b-0a46-41ad-a418-1c1ec3f1410f",
   "metadata": {},
   "source": [
    "# Scoring Script Test\n",
    "The purpose of this test is a proof of concept implementation of NannyML performance estimation into the bridging scoring script. This could then be used to analyse data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bf2acb-3b0a-4a9c-b256-420ca0609ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt;\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from pom_NEW import *\n",
    "import os\n",
    "import sys\n",
    "import gcsfs\n",
    "sys.path.append(os.path.abspath(\"/home/jupyter/POM-feature-drift\"))\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import project_config as pc\n",
    "import common_variables as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3da25c-8cf2-47f2-91ea-89f61ba6bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = 'offer_bridging_eoo_base'\n",
    "bucket_location = 'EU'\n",
    "bucket_id       = 'gs://'+pc.bucket+'/pom_scoring'\n",
    "file_name       = 'eoo_base_' + str(dt.datetime.now().date())\n",
    "file_format     = 'CSV'\n",
    "gcs_file_path   = os.path.join(bucket_id,file_name+'_*.csv')\n",
    "prefix_name = 'pom_scoring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed48696a-73b4-408e-9e50-fd1d65362f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connection to BQ\n",
    "client = bigquery.Client(project=pc.project_id) #;\n",
    "query = \"\"\" SELECT column_name, data_type\n",
    "FROM \"\"\"+pc.target_dataset+\"\"\".INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE TABLE_NAME = 'offer_bridging_eoo_base'\n",
    "AND data_type = 'DATE'\n",
    "\"\"\"\n",
    "\n",
    "date_cols = client.query(query).to_dataframe().iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e35eeb-e53a-42a6-9eb5-4e80ee9d8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load columns used in new models - speeds up process as we don't need to keep unused columns\n",
    "cols = {'Account_Number', 'base_dt', 'eoo_base_obs_dt', 'Cohort', 'Rack_Rate', 'Ttl_Offer_Discount', 'Customer_Type', 'Country'}\n",
    "for customer_type in ['DTV']:\n",
    "    for country in ['UK']:\n",
    "        for target_type in ['arpu', 'churn', 'ta']:\n",
    "            for model_type in ['NT', 'L', 'M', 'H']:\n",
    "                pickle_name = f'pickle_files/{customer_type}_{country}_{target_type}_{model_type}.pkl'\n",
    "                if os.path.isfile(pickle_name):\n",
    "                    with open(pickle_name, 'rb') as pickle_file:\n",
    "                        model = pickle.load(pickle_file)\n",
    "                    if (type(model) is XGBRegressor) or (type(model) is XGBClassifier):\n",
    "                        model_columns = model.get_booster().feature_names\n",
    "                    elif (type(model) is LGBMRegressor) or (type(model) is LGBMClassifier):\n",
    "                        model_columns = model.feature_name_\n",
    "                    elif (type(model) is CombinedModel):\n",
    "                        model_columns = model.feature_name_\n",
    "                    else:\n",
    "                        print(f'MODEL TYPE NOT MATCHED {pickle_name}')\n",
    "                        model_columns = []\n",
    "                    if model_columns is None:\n",
    "                        print(f'NONE COLUMNS {pickle_name}')\n",
    "                        model_columns = []\n",
    "                    cols = cols.union(model_columns)\n",
    "rename_andrew = fix_columns()\n",
    "cols = cols.union({x for x, y in rename_andrew.items() if y in cols})\n",
    "cols = cols.union({x.rsplit('_', 1)[0] for x in cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de271407-a6b0-4f67-8d24-5fcd6652b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobs are ['pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000000.csv', 'pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000001.csv', 'pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000002.csv', 'pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000003.csv', 'pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000004.csv', 'pom-etl-process/pom_scoring/eoo_base_2022-12-05_000000000005.csv']\n",
      "Total # of date variables are 167\n",
      "Inital dataframe contains 456831 rows and 987 columns\n",
      "Shape of the dataset is 456831 rows and 987 columns\n"
     ]
    }
   ],
   "source": [
    "#Load scoring data from GCS to pandas dataframe\n",
    "df_main = read_pomdata_to_score(project = pc.project_id, bucket_name = pc.bucket, prefix_name = prefix_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d59ab-a77f-4c54-803e-d3c8ee23c818",
   "metadata": {},
   "source": [
    "## Clean EOO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9861430e-eb94-475e-9689-9cd7505b5a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eoo_base_obs_dt available in the date df\n"
     ]
    }
   ],
   "source": [
    "dpp = DataPreProcess(df=df_main.rename(columns={'EOO_Base_Obs_Dt' : 'eoo_base_obs_dt'}), cols=cols, date_cols=date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77033d63-d8df-4a0a-887a-c4eb78e5309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d387c5ac-4282-4a09-b180-ecbeb5ee7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dpp.process_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf494652-db8b-43dc-b0e2-05f2125c85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _ = dpp.fill_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b722dfa-8527-4a58-87ba-b13a73d897f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dpp.scale_numeric(excl_cols=['Account_Number', 'Movies_Active', 'Sports_Active', 'SGE_Active', 'HD_Active', 'MS_Active'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cfb022-a385-43e4-afeb-e5b9d3a5159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "_ = dpp.one_hot_encode(nunique=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079fb4e8-dacf-4717-be8f-154dba1f0889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains the total 456831 rows and 265 columns\n",
      "Data contains the total 456831 rows and 265 columns\n"
     ]
    }
   ],
   "source": [
    "data_consolidated = dpp.concat_data(column_fix={'HD_Product_Holding_nan' : 'HD_Product_Holding_None'})\n",
    "data_consolidated_scaled = dpp.concat_data(scale_numeric=True, column_fix=rename_andrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fc0e04-6a78-40c1-b924-a81f6144a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consolidated_uk_dtv = data_consolidated.loc[(data_consolidated['Country_UK'] == 1) & (data_consolidated['Customer_Type'] == 'DTV')]\n",
    "data_consolidated_uk_dtv_scaled = data_consolidated_scaled.loc[(data_consolidated_scaled['Country_UK'] == 1) & (data_consolidated_scaled['Customer_Type'] == 'DTV')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabb7022-d4f2-474f-bd88-7a8a88cdb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3299b6-3408-4580-9aa8-9381055debf5",
   "metadata": {},
   "source": [
    "## Score TA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b6e156-3e8a-4130-b388-0e5fc9e9cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************Model file DTV_UK_ta_NT exists************************\n",
      "************************Scored data for model DTV_UK_ta_NT************************\n",
      "************************Model file DTV_UK_ta_L exists************************\n",
      "************************Scored data for model DTV_UK_ta_L************************\n",
      "************************Model file DTV_UK_ta_M exists************************\n",
      "************************Scored data for model DTV_UK_ta_M************************\n",
      "************************Model file DTV_UK_ta_H exists************************\n",
      "************************Scored data for model DTV_UK_ta_H************************\n"
     ]
    }
   ],
   "source": [
    "data_dict_uk_dtv_stepup = score_data(df=data_consolidated_uk_dtv_scaled, customer_type='DTV', country='UK', target='target_ta', model_types=stepup_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d8b50c3-7fb9-476d-be88-c6d5dd036fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_df_ta_uk_dtv_new = pd.concat(list(data_dict_uk_dtv_stepup.values()), axis=1)\n",
    "scored_df_ta = pd.concat([scored_df_ta_uk_dtv_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a43372-cc1d-448d-b901-0a1b1548c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scored_df_ta_uk_dtv_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6c610-8211-4fd3-87d2-1fb110e6eec0",
   "metadata": {},
   "source": [
    "## NannyML Performance Estimation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad250b7-1d97-4af2-8ef6-2a2605ef27c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
