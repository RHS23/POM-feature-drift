{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa3f5cc-2823-47a4-9cb0-050cb70a0d60",
   "metadata": {},
   "source": [
    "# Reference Data Creation\n",
    "Here we will create the reference data for use in our NannyML test notebooks.\n",
    "First we will create the data for the scoring script test. This will be 'full' validation reference data, meaning it will be comprised of all five validation cohorts (to try and replecate how it would be implemented in production).\n",
    "Then we will create reference data for the validation test that will comprise of the first three validation cohorts. The reason for this is that we can use the final two cohorts as analysis data to see how the performance estimation works.\n",
    "Here we will also create pickled drift artefacts for use in the previously mentioned tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1c78ba-6d15-4b8e-81fd-9fbab1ff40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt;\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import os\n",
    "import sys\n",
    "import gcsfs\n",
    "sys.path.append(os.path.abspath(\"/home/jupyter/POM-feature-drift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fdb18-3967-4036-a076-433c0be420b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Script Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327e035-106f-4625-b803-c2384feff984",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87d5de7e-0c37-4700-9aba-5cc7c9593636",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111', '202116-202119', '292124-202127']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/ta_step_up_{treatment}_v2_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgb_preds', 'xgb_proba', 'logr_preds', 'logr_proba'], inplace = True)\n",
    "    df.rename(columns = {'lgbm_preds': 'pred_ta', 'lgbm_proba': 'pred_proba_ta'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'ta_{treatment}_reference_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c406-0b7a-486c-989d-e2a57a8fd954",
   "metadata": {},
   "source": [
    "## ARPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d77bcaf-f903-4547-a0e5-d04997338690",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111', '202116-202119', '292124-202127']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/revenue_step_up_{treatment}_6m_v3_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgbr_preds', 'linr_preds'], inplace = True)\n",
    "    df.rename(columns = {'lgbmr_preds': 'pred_arpu'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'arpu_{treatment}_reference_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0d636-a3aa-4c26-8f73-45e3c6f27c02",
   "metadata": {},
   "source": [
    "## ARPU Performance Estimation\n",
    "We can pre-fit and pickle the regression performance estimators and then just open and run them on new data in the scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e6638f-17d0-4f47-b58a-85e558a5f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nannyml as nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1115da-e3b0-4e1a-b252-a380f43952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']\n",
    "arpu_model_dict = {}\n",
    "for treatment in stepup_models:\n",
    "    with open(f'/home/jupyter/POM-feature-drift/pickle_files/DTV_UK_arpu_{treatment}.pkl', 'rb') as pkl_file:\n",
    "        arpu_model_dict[treatment] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36051eef-e749-4858-abfe-cbfe5512bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref, treatment in zip(['nt', 'low', 'medium', 'high'], stepup_models):   \n",
    "    ref_data = pd.read_csv(f'arpu_{ref}_reference_full.csv', low_memory = False)\n",
    "    ref_data['timestamp'] = pd.to_datetime(ref_data['timestamp'])\n",
    "    ref_data.sort_values(by = 'timestamp', inplace = True)\n",
    "    features = arpu_model_dict[treatment].feature_name_\n",
    "\n",
    "    estimator = nml.DLE(feature_column_names = features,\n",
    "                     y_pred = 'pred_arpu',\n",
    "                     y_true = 'target_arpu',\n",
    "                     metrics = ['rmse', 'mae'],\n",
    "                     timestamp_column_name = 'timestamp',\n",
    "                     # chunk_period = 'W',\n",
    "                     chunk_number = 10,\n",
    "                    )\n",
    "\n",
    "    estimator = estimator.fit(ref_data[features + ['pred_arpu', 'target_arpu', 'timestamp']])\n",
    "                                                            \n",
    "    with open(f'NannyML_results/arpu_{ref}_perf_estimator.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(estimator, pkl_file, protocol = pickle.HIGHEST_PROTOCOL)                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33084072-51be-4c96-9247-7bd2883be5db",
   "metadata": {},
   "source": [
    "# Validation Tests Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d94fe-be51-40e6-8272-841c4190942f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c58a5-738a-4a29-af9f-5103096a984c",
   "metadata": {},
   "source": [
    "### Performance Estimation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371596e9-d90c-48b4-9dea-6cd8f21f123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/ta_step_up_{treatment}_v2_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgb_preds', 'xgb_proba', 'logr_preds', 'logr_proba'], inplace = True)\n",
    "    df.rename(columns = {'lgbm_preds': 'pred_ta', 'lgbm_proba': 'pred_proba_ta'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'ta_{treatment}_reference_validation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7e01e-2192-4fe1-937c-6ed545c0c7c3",
   "metadata": {},
   "source": [
    "### Data Drift Calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be39391e-4e79-4b1a-b2c8-d462b1f0935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']\n",
    "ta_model_dict = {}\n",
    "for treatment in stepup_models:\n",
    "    with open(f'/home/jupyter/POM-feature-drift/pickle_files/DTV_UK_ta_{treatment}.pkl', 'rb') as pkl_file:\n",
    "        ta_model_dict[treatment] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b18a5b0f-4324-4468-bc38-ef773b18c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment, model in zip(stepup_models, ['nt', 'low', 'medium', 'high']):\n",
    "    # Read in full five cohort validation data for analysis\n",
    "    analysis = pd.read_csv(f'ta_{model}_reference_full.csv', low_memory = False)\n",
    "    analysis.timestamp = pd.to_datetime(analysis.timestamp)\n",
    "    features = ta_model_dict[treatment].feature_name_\n",
    "    \n",
    "    # Read in scored training data as referenece\n",
    "    df = pd.read_csv(f'gs://andrew-pom/Revenue/ta_step_up_{model}_v2_train_scored.csv', low_memory = False)\n",
    "    \n",
    "    # Setup reference data for use (timestamp, drop/rename columns, sort)\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgb_preds', 'xgb_proba', 'logr_preds', 'logr_proba'], inplace = True)\n",
    "    df.rename(columns = {'lgbm_preds': 'pred_ta', 'lgbm_proba': 'pred_proba_ta'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    # Initialise NannyML drift calculator\n",
    "    calculator = nml.UnivariateDriftCalculator(column_names = features + ['pred_proba_ta'],\n",
    "                                                      timestamp_column_name = 'timestamp',\n",
    "                                                      chunk_period = 'W',\n",
    "                                                      continuous_methods = ['jensen_shannon'],\n",
    "                                                      categorical_methods = ['jensen_shannon'])\n",
    "    \n",
    "    # Fit calculator to reference data\n",
    "    calculator = calculator.fit(df[features + ['timestamp', 'pred_proba_ta']])\n",
    "    \n",
    "    # Pickle and save calculator so it can be loaded and used when needed\n",
    "    with open(f'NannyML_results/ta_{model}_drift_calculator.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(calculator, pkl_file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8ca6-f159-416c-b862-e573d148db64",
   "metadata": {},
   "source": [
    "## ARPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efa563-c081-40b8-8d36-39168a1dff59",
   "metadata": {},
   "source": [
    "### Performance Estimation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ada0a7c9-c82a-400a-8c9d-2b11c3f9dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/revenue_step_up_{treatment}_6m_v3_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgbr_preds', 'linr_preds'], inplace = True)\n",
    "    df.rename(columns = {'lgbmr_preds': 'pred_arpu'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'arpu_{treatment}_reference_validation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e2714-d522-444f-8be5-a7344172080e",
   "metadata": {},
   "source": [
    "### Performance Estimation Calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cf2903a-6282-4e78-96f3-14eb3845ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']\n",
    "arpu_model_dict = {}\n",
    "for treatment in stepup_models:\n",
    "    with open(f'/home/jupyter/POM-feature-drift/pickle_files/DTV_UK_arpu_{treatment}.pkl', 'rb') as pkl_file:\n",
    "        arpu_model_dict[treatment] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8111e525-688b-49dc-9af4-dbd63179b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref, treatment in zip(['nt', 'low', 'medium', 'high'], stepup_models):   \n",
    "    ref_data = pd.read_csv(f'arpu_{ref}_reference_validation.csv', low_memory = False)\n",
    "    ref_data['timestamp'] = pd.to_datetime(ref_data['timestamp'])\n",
    "    ref_data.sort_values(by = 'timestamp', inplace = True)\n",
    "    features = arpu_model_dict[treatment].feature_name_\n",
    "\n",
    "    estimator = nml.DLE(feature_column_names = features,\n",
    "                     y_pred = 'pred_arpu',\n",
    "                     y_true = 'target_arpu',\n",
    "                     metrics = ['rmse', 'mae'],\n",
    "                     timestamp_column_name = 'timestamp',\n",
    "                     # chunk_period = 'W',\n",
    "                     chunk_number = 10,\n",
    "                    )\n",
    "\n",
    "    estimator = estimator.fit(ref_data[features + ['pred_arpu', 'target_arpu', 'timestamp']])\n",
    "                                                            \n",
    "    with open(f'NannyML_results/arpu_{ref}_perf_estimator_validation.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(estimator, pkl_file, protocol = pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c213b3-60c4-47eb-9431-056a17cdb5e9",
   "metadata": {},
   "source": [
    "### Data Drift Calculators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d30a314a-1b21-4a11-a15a-d5b9115cb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']\n",
    "arpu_model_dict = {}\n",
    "for treatment in stepup_models:\n",
    "    with open(f'/home/jupyter/POM-feature-drift/pickle_files/DTV_UK_arpu_{treatment}.pkl', 'rb') as pkl_file:\n",
    "        arpu_model_dict[treatment] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee9e9d10-a7ec-4f94-8634-6a3a9e28cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment, model in zip(stepup_models, ['nt', 'low', 'medium', 'high']):\n",
    "    # Read in full five cohort validation data for analysis\n",
    "    analysis = pd.read_csv(f'arpu_{model}_reference_full.csv', low_memory = False)\n",
    "    analysis.timestamp = pd.to_datetime(analysis.timestamp)\n",
    "    features = arpu_model_dict[treatment].feature_name_\n",
    "    \n",
    "    # Read in scored training data as referenece\n",
    "    df = pd.read_csv(f'gs://andrew-pom/Revenue/revenue_step_up_{model}_6m_v3_train_scored.csv', low_memory = False)\n",
    "    \n",
    "    # Setup reference data for use (timestamp, drop/rename columns, sort)\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgbr_preds', 'linr_preds'], inplace = True)\n",
    "    df.rename(columns = {'lgbmr_preds': 'pred_arpu'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    # Initialise NannyML drift calculator\n",
    "    calculator = nml.UnivariateDriftCalculator(column_names = features + ['pred_arpu'],\n",
    "                                                      timestamp_column_name = 'timestamp',\n",
    "                                                      chunk_period = 'W',\n",
    "                                                      continuous_methods = ['jensen_shannon'],\n",
    "                                                      categorical_methods = ['jensen_shannon'])\n",
    "    \n",
    "    # Fit calculator to reference data\n",
    "    calculator = calculator.fit(df[features + ['timestamp', 'pred_arpu']])\n",
    "    \n",
    "    # Pickle and save calculator so it can be loaded and used when needed\n",
    "    with open(f'NannyML_results/arpu_{model}_drift_calculator.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(calculator, pkl_file, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f88d86-94f0-4fe0-b834-62e42938d829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
