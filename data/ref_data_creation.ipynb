{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa3f5cc-2823-47a4-9cb0-050cb70a0d60",
   "metadata": {},
   "source": [
    "# Reference Data Creation\n",
    "Here we will create the reference data for use in our NannyML test notebooks.\n",
    "First we will create the data for the scoring script test. This will be 'full' validation reference data, meaning it will be comprised of all five validation cohorts (to try and replecate how it would be implemented in production).\n",
    "Then we will create reference data for the validation test that will comprise of the first three validation cohorts. The reason for this is that we can use the final two cohorts as analysis data to see how the performance estimation works.\n",
    "Here we will also create pickled drift artefacts for use in the previously mentioned tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1c78ba-6d15-4b8e-81fd-9fbab1ff40d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import matplotlib.pyplot as plt;\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import os\n",
    "import sys\n",
    "import gcsfs\n",
    "sys.path.append(os.path.abspath(\"/home/jupyter/POM-feature-drift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7fdb18-3967-4036-a076-433c0be420b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scoring Script Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327e035-106f-4625-b803-c2384feff984",
   "metadata": {},
   "source": [
    "## TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87d5de7e-0c37-4700-9aba-5cc7c9593636",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111', '202116-202119', '292124-202127']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/ta_step_up_{treatment}_v2_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgb_preds', 'xgb_proba', 'logr_preds', 'logr_proba'], inplace = True)\n",
    "    df.rename(columns = {'lgbm_preds': 'pred_ta', 'lgbm_proba': 'pred_proba_ta'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'ta_{treatment}_reference_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c406-0b7a-486c-989d-e2a57a8fd954",
   "metadata": {},
   "source": [
    "## ARPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d77bcaf-f903-4547-a0e5-d04997338690",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_cohorts = ['202045-202052', '202053-202107', '202108-202111', '202116-202119', '292124-202127']\n",
    "for treatment in ['nt', 'low', 'medium', 'high']:\n",
    "    df = pd.DataFrame()\n",
    "    for cohort in validation_cohorts:\n",
    "        gcs_path = f'gs://andrew-pom/Revenue/revenue_step_up_{treatment}_6m_v3_{cohort}_oot.csv'\n",
    "        cohort_df = pd.read_csv(gcs_path, low_memory = False)\n",
    "        df = pd.concat([df, cohort_df])\n",
    "\n",
    "    dates = []\n",
    "    for index in df.ind:\n",
    "        dates.append(index.split('-')[0])\n",
    "\n",
    "    timestamps = []\n",
    "    for i, date in enumerate(dates):\n",
    "        timestamps.append(pd.to_datetime(str(dates[i]) + '-0', format = '%Y%W-%w'))\n",
    "    df['timestamp'] = timestamps\n",
    "\n",
    "    df.drop(columns = ['ind', 'xgbr_preds', 'linr_preds'], inplace = True)\n",
    "    df.rename(columns = {'lgbmr_preds': 'pred_arpu'}, inplace = True)\n",
    "    df.sort_values(by = 'timestamp', inplace = True)\n",
    "    \n",
    "    df.to_csv(f'arpu_{treatment}_reference_full.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0d636-a3aa-4c26-8f73-45e3c6f27c02",
   "metadata": {},
   "source": [
    "## ARPU Performance Estimation\n",
    "We can pre-fit and pickle the regression performance estimators and then just open and run them on new data in the scoring script. This is good to do as they take significantly longer to fit than the classification versions (due to the complexity of the algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e6638f-17d0-4f47-b58a-85e558a5f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nannyml as nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1115da-e3b0-4e1a-b252-a380f43952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepup_models = ['NT', 'L', 'M', 'H']\n",
    "arpu_model_dict = {}\n",
    "for treatment in stepup_models:\n",
    "    with open(f'/home/jupyter/POM-feature-drift/pickle_files/DTV_UK_arpu_{treatment}.pkl', 'rb') as pkl_file:\n",
    "        arpu_model_dict[treatment] = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36051eef-e749-4858-abfe-cbfe5512bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref, treatment in zip(['nt', 'low', 'medium', 'high'], stepup_models):   \n",
    "    ref_data = pd.read_csv(f'arpu_{ref}_reference_full.csv', low_memory = False)\n",
    "    ref_data['timestamp'] = pd.to_datetime(ref_data['timestamp'])\n",
    "    ref_data.sort_values(by = 'timestamp', inplace = True)\n",
    "    features = arpu_model_dict[treatment].feature_name_\n",
    "\n",
    "    estimator = nml.DLE(feature_column_names = features,\n",
    "                     y_pred = 'pred_arpu',\n",
    "                     y_true = 'target_arpu',\n",
    "                     metrics = ['rmse', 'mae'],\n",
    "                     timestamp_column_name = 'timestamp',\n",
    "                     # chunk_period = 'W',\n",
    "                     chunk_number = 10000,\n",
    "                    )\n",
    "\n",
    "    estimator = estimator.fit(ref_data[features + ['pred_arpu', 'target_arpu', 'timestamp']])\n",
    "                                                            \n",
    "    with open(f'NannyML_results/arpu_{ref}_perf_estimator.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(estimator, pkl_file, protocol = pickle.HIGHEST_PROTOCOL)                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bb7db-ed51-47c7-b5dd-d8eb97a09645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "local-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
